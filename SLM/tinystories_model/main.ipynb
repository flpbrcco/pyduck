{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a04765",
   "metadata": {},
   "source": [
    "Large language models (LLMs) like GPT-3 are resource-intensive, requiring massive datasets and computational power. Small language models, however, offer a practical alternative for learning and experimentation on standard hardware. In this article I will be using the TinyStories dataset (~2 million training stories, ~20,000 validation stories) to build an SLM, using PyTorch for model implementation and Hugging Faceâ€™s tiktoken for tokenization. The code is optimized for efficiency, using techniques like binary file storage and GPU memory management to handle large datasets without overwhelming system resources.\n",
    "\n",
    "    The goal is to create a model that can generate coherent, short stories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99b6dc",
   "metadata": {},
   "source": [
    "small language model (SLM) using the TinyStories and Encyclopedia dataset. Designed to be educational, the notebook covers data loading, tokenization, efficient storage, batch generation, and model training with a custom Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is the code to install necessary dependencies\n",
    "!pip install -U datasets\n",
    "!pip install tiktoken\n",
    "!pip install numpy matplotlib tqdm\n",
    "!pip install accelerate wandb tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinystories_model (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
